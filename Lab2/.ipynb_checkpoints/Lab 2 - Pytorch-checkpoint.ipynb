{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0114eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661d368",
   "metadata": {
    "colab_type": "text",
    "id": "mLOCk6P-fldm"
   },
   "source": [
    "## Task I: Word-based CNN for Text Classification\n",
    "\n",
    "### 1. Data\n",
    "\n",
    "The dataset that we are going to use is the imdb dataset of movie reviews. These are labelled by sentiment (positive/negative). \n",
    "\n",
    "The reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "\n",
    "For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "More information regarding the dataset can be found in the official [documentation](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2edd1c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_samples, train_labels, test_samples, test_labels = pickle.load(open('./imdb.pkl', 'rb'))\n",
    "\n",
    "print(len(train_samples), 'train sequences')\n",
    "print(len(test_samples), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ed573",
   "metadata": {
    "colab_type": "text",
    "id": "iWg-G68bh2uI"
   },
   "source": [
    "### 2. Preprocess the text data \n",
    "\n",
    "In this particular case, where we are using the imdb dataset there is no need to do all the traditional preprocessings that we normally do when dealing with NLP problems. Part of them are already done at this point.\n",
    "\n",
    "  - Split the dataset in train and test (maybe also validation).\n",
    "  - Tokenize and transform to integer index. Here we would need to: \n",
    "    - instantitate a *Tokenizer()* object, \n",
    "    - fit that object on the text on which we are training the model (use the *fit_on_texts()* method)\n",
    "    - call *texts_to_sequences()* for both the training and the test text.\n",
    "\n",
    "  - **Add padding to ensure that all vectors have the same dimensionality.** Note that this is the only pre-processing that needs to be done in the case of the current imdb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b345b0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n",
      "141\n",
      "550\n",
      "147\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "train_samples shape: (25000,)\n",
      "test_samples shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "desired_length = 1000\n",
    "\n",
    "# TODO 2. Pad sequences\n",
    "\n",
    "# define a function that receives a set of samples and a desired length and:\n",
    "# 1. cuts all sequences longer than the desired length to the desired length\n",
    "# (one improvement would be to perform this action in the torch dataset and feed a random subseqence \n",
    "# of the desired length each time)\n",
    "# 2. pad shorter sequences with the value 10001\n",
    "\n",
    "def pad(samples, length):\n",
    "    for i,sample in enumerate(samples):\n",
    "        if len(sample) > desired_length:\n",
    "            samples[i] = sample[0:desired_length]\n",
    "        else:\n",
    "            samples[i] = sample + [1001]*(desired_length - len(sample))\n",
    "    return samples\n",
    "for i in range(5):\n",
    "    print(len(train_samples[i]))\n",
    "\n",
    "train_samples = pad(train_samples, desired_length)\n",
    "test_samples = pad(test_samples, desired_length)\n",
    "for i in range(5):\n",
    "    print(len(train_samples[i]))\n",
    "print('train_samples shape:', train_samples.shape)\n",
    "print('test_samples shape:', test_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa83576",
   "metadata": {
    "colab_type": "text",
    "id": "DtIeukSXltKp"
   },
   "source": [
    "### 3.  Define the model de dataset and the training loop\n",
    "\n",
    "Similar to the privious lab while following the model architecture described in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f749dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer with a vocabulary size of 10002\n",
    "        # an output embedding size of 100\n",
    "        # and a padding_idx equal to the one used - 10001\n",
    "        self.embedding = torch.nn.Embedding(10002, 100,10001)\n",
    "\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            \n",
    "        # Define the following sequence of layers\n",
    "        \n",
    "        # A dropout layer with a probability of 0.4\n",
    "        \n",
    "            torch.nn.Dropout(0.4),\n",
    "        # A 1D Convolutional layer with 100 input channels, 128 output channels, kernel size of 3 and a padding of 1\n",
    "        # A 1D Batch Normalization Layer for 128 features\n",
    "        # A ReLU activation\n",
    "        # A 1D Maxpooling layer with size 2\n",
    "            torch.nn.Conv1d(100, 128, 3, 1),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "\n",
    "        # A 1D Convolutional layer with 128 input channels, 128 output channels, kernel size of 5 and a padding of 2\n",
    "        # A 1D Batch Normalization Layer for 128 features\n",
    "        # A ReLU activation\n",
    "        # A 1D Maxpooling layer with size 2\n",
    "            torch.nn.Conv1d(128, 128, 5, 2),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(2),\n",
    "        \n",
    "        # A 1D Convolutional layer with 128 input channels, 128 output channels, kernel size of 5 and a padding of 2\n",
    "        # A 1D Batch Normalization Layer for 128 features\n",
    "        # A ReLU activation\n",
    "        # A 1D Maxpooling layer with size 2\n",
    "            torch.nn.Conv1d(128, 128, 5, 2),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(2),\n",
    "        # A global Average pooling layer, which in this scenario, will be an 1D Avgerage Pooling layer\n",
    "        # with size 125 and stride 125\n",
    "            torch.nn.AvgPool1d(125,125),\n",
    "        # A Flattening layer\n",
    "            torch.nn.Flatten(),\n",
    "        # A Linear layer with 128 input features and 2 outputs and no activation function\n",
    "            torch.nn.Linear(128, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # forward the input through the embedding layer\n",
    "        out = self.embedding(input)\n",
    "        # permute the input such that it becomes channels first\n",
    "        out.permute(0,2,1)\n",
    "        # forward the input through the rest of the sequence of layers\n",
    "        out = self.seq(out)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98d6d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# define a dataset class that feeds the samples from train or test, depending on the use case\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, use = 'train'):\n",
    "        if use == 'train':\n",
    "            \n",
    "            self.data = train_samples\n",
    "            self.labels = train_labels\n",
    "        elif use == 'test':\n",
    "            self.data = test_samples\n",
    "            self.labels = test_labels\n",
    "            \n",
    "    def __getitem__(self, sample_n):\n",
    "        return self.data[sample_n], self.labels[sample_n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "# instantiate the model\n",
    "model = Model()\n",
    "# define an Adam optimizer for the model with a lr of 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "# define a Cross Entropy loss function\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define the training dataset and dataloader and the test dataset and dataloader\n",
    "train_ds = Dataset()\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=8)\n",
    "test_ds = Dataset(use='test')\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=8)\n",
    "\n",
    "# write the training loop as defined in Lab 1 and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02511c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_func, dl, epochs):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
