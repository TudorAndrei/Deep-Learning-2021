{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea95b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6065e",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "The best place to access books that are no longer under Copyright is [Project Gutenberg](https://www.gutenberg.org/). Today we recommend using [Aliceâ€™s Adventures in Wonderland by Lewis Carroll](https://www.gutenberg.org/files/11/11-0.txt) for consistency. Of course you can experiment with other books as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038f37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://www.gutenberg.org/files/219/219-0.txt'\n",
    "fname = 'heart_of_darkness.txt'\n",
    "\n",
    "if fname not in os.listdir():\n",
    "    urllib.request.urlretrieve(data_url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae584b2e",
   "metadata": {},
   "source": [
    "## Load data and create character to integer mappings\n",
    "\n",
    "- Open the text file, read the data then convert it to lowercase letters.\n",
    "- Map each character to a respective number. Keep 2 dictionaries in order to have more easily access to the mappings both ways around.\n",
    "- Transform the data from a list of characters to a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "569cf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import itertools\n",
    "with open(fname, 'r') as f:\n",
    "    txt = txt = f.read().lower()\n",
    "# print(txt)\n",
    "# # print(\" \".join(txt))\n",
    "id2word = {idx: word for idx, word in enumerate(set(txt))}\n",
    "word2id = {word: idx for idx, word in enumerate(set(txt))}\n",
    "# # Characters to integers\n",
    "txt_ids = [word2id[word] for word in txt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b691e4",
   "metadata": {},
   "source": [
    "## Define the datasets and dataloaders\n",
    "- We are \"thinking\" in sequences of 100 characters: 99 characters in the input and 1 in the output.  \n",
    "E.g. for the sequence *\\['h', 'e', 'l', 'l'\\]* as input, we will have *\\['o'\\]* as the expected output.\n",
    "- Each pair (sample, label) from the training dataset will be composed from a sequence of 99 ints and a single integer label\n",
    "- We will keep the first 85% sequences as training data and use the remaining for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4dbf17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt, mode='train'):\n",
    "        train_size = int(len(txt) * 0.85)\n",
    "        if mode == 'train':\n",
    "            self.txt = txt[0: train_size]\n",
    "        else:\n",
    "            self.txt = txt[train_size: ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.txt) - 99\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.txt[idx : idx + 99], self.txt[idx + 99]\n",
    "\n",
    "train_dataset = TextDataset(txt_ids)\n",
    "val_dataset = TextDataset(txt_ids, mode='val')\n",
    "# Define dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd6868",
   "metadata": {},
   "source": [
    "## Define a model with\n",
    "- An embedding layer with size 32\n",
    "- Three LSTM layers with a hidden size of 256 and a dropout rate of 20%\n",
    "- A final linear classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(32,vocab_size)\n",
    "        self.lstm1 =  torch.nn.LSTM(256, dropout=.2)\n",
    "        self.lstm2 =  torch.nn.LSTM(256, dropout=.2)\n",
    "        self.lstm3 =  torch.nn.LSTM(256, dropout=.2)\n",
    "        self.classif = torch.nn.Softmax(vocab_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.emb(input)\n",
    "        out = self.lstm1(out)\n",
    "        out = self.lstm2(out)\n",
    "        out = self.lstm3(out)\n",
    "        return self.classif(out)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea7b59",
   "metadata": {},
   "source": [
    "## Define the training loop and train the model to predict the next character in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8d74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define the training loop and traing the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7c599",
   "metadata": {},
   "source": [
    "## Evaluate the model by generating text\n",
    "\n",
    "- Start with 99 characters (potentially chosen from a text)\n",
    "- Generate a new character using the trained network\n",
    "- Repeat the process by appending the generated character and making a prediction for a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
