{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea95b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6065e",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "The best place to access books that are no longer under Copyright is [Project Gutenberg](https://www.gutenberg.org/). Today we recommend using [Aliceâ€™s Adventures in Wonderland by Lewis Carroll](https://www.gutenberg.org/files/11/11-0.txt) for consistency. Of course you can experiment with other books as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038f37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://www.gutenberg.org/files/219/219-0.txt'\n",
    "fname = 'heart_of_darkness.txt'\n",
    "\n",
    "if fname not in os.listdir():\n",
    "    urllib.request.urlretrieve(data_url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae584b2e",
   "metadata": {},
   "source": [
    "## Load data and create character to integer mappings\n",
    "\n",
    "- Open the text file, read the data then convert it to lowercase letters.\n",
    "- Map each character to a respective number. Keep 2 dictionaries in order to have more easily access to the mappings both ways around.\n",
    "- Transform the data from a list of characters to a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "569cf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import itertools\n",
    "with open(fname, 'r') as f:\n",
    "    txt = txt = f.read().lower()\n",
    "# print(txt)\n",
    "# # print(\" \".join(txt))\n",
    "id2word = {idx: word for idx, word in enumerate(set(txt))}\n",
    "word2id = {word: idx for idx, word in enumerate(set(txt))}\n",
    "# # Characters to integers\n",
    "txt_ids = [word2id[word] for word in txt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b691e4",
   "metadata": {},
   "source": [
    "## Define the datasets and dataloaders\n",
    "- We are \"thinking\" in sequences of 100 characters: 99 characters in the input and 1 in the output.  \n",
    "E.g. for the sequence *\\['h', 'e', 'l', 'l'\\]* as input, we will have *\\['o'\\]* as the expected output.\n",
    "- Each pair (sample, label) from the training dataset will be composed from a sequence of 99 ints and a single integer label\n",
    "- We will keep the first 85% sequences as training data and use the remaining for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dbf17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt, mode='train'):\n",
    "        train_size = int(len(txt) * 0.85)\n",
    "        if mode == 'train':\n",
    "            self.txt = txt[0: train_size]\n",
    "        else:\n",
    "            self.txt = txt[train_size: ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.txt) - 99\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return np.array(self.txt[idx : idx + 99]), self.txt[idx + 99]\n",
    "\n",
    "train_dataset = TextDataset(txt_ids)\n",
    "val_dataset = TextDataset(txt_ids, mode='val')\n",
    "# Define dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd6868",
   "metadata": {},
   "source": [
    "## Define a model with\n",
    "- An embedding layer with size 32\n",
    "- Three LSTM layers with a hidden size of 256 and a dropout rate of 20%\n",
    "- A final linear classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c45f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size,32)\n",
    "        self.lstm1 =  torch.nn.LSTM(32, 100, num_layers = 3, dropout=.2)\n",
    "        self.classif = torch.nn.Linear(100,vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.emb(x).permute(1,0,2)\n",
    "        \n",
    "        out = self.lstm1(out)[0][-1]\n",
    "#         print(out)\n",
    "        return self.classif(out)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea7b59",
   "metadata": {},
   "source": [
    "## Define the training loop and train the model to predict the next character in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed8d74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_195309/3134105066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dlu/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_195309/1772556131.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#         print(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dlu/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dlu/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    680\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the training loop and traing the model\n",
    "model = Model(len(id2word))\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "lossf = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for x, y in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        y_hat = model(x.long())\n",
    "        loss = lossf(y_hat, y)\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7c599",
   "metadata": {},
   "source": [
    "## Evaluate the model by generating text\n",
    "\n",
    "- Start with 99 characters (potentially chosen from a text)\n",
    "- Generate a new character using the trained network\n",
    "- Repeat the process by appending the generated character and making a prediction for a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
