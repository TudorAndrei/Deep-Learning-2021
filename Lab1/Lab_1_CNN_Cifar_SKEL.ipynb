{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vnji51WlKaBD"
   },
   "source": [
    "# Lab 1: Convolutional Neural Networks\n",
    "\n",
    "This notebook contains the key concepts and steps to implementing a Convolutional Neural Network.\n",
    "Implementing the tasks below should give you an idea of how this method for image classification works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4QscxpVtigB"
   },
   "source": [
    "## STEP 0: Get an idea about the dataset\n",
    "\n",
    "### Nothing to be done here\n",
    "\n",
    "**CIFAR** - Canadian Institute For Advanced Research.\n",
    "\n",
    "CIFAR-10 and CIFAR-100 datasets were developed by the researchers at the CIFAR institute.\n",
    "\n",
    "This dataset has 60.000 color (depth 3) images 32x32 (WxH) of objects from 10 classes:\n",
    "- 0: airplane\n",
    "- 1: automobile\n",
    "- 2: bird\n",
    "- 3: cat\n",
    "- 4: deer\n",
    "- 5: dog\n",
    "- 6: frog\n",
    "- 7: horse\n",
    "- 8: ship\n",
    "- 9: truck.\n",
    "\n",
    "These are really small images and suitable for the purposes of this lab: we want to learn \n",
    "how to implement a convolutional neural network, without painfully waiting for the training to end.\n",
    "\n",
    "The images are indeed very small compared to modern photographs. It can be challenging to see what exactly is represented in some of them. This low resolution is likely the cause of the limited performance that top-of-the-line algorithms are able to achieve on the dataset.\n",
    "\n",
    "On this dataset it is relatively straightforward to achieve 80% classification accuracy.\n",
    "Using deep CNNs a top performance above 90% accuracy is achieved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZ7iOuwottzn"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_151202/2483442853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# download cifar data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# summarize the loaded dataset\n",
    "# 50,000 training samples and 10,000 test samples (size: 32x32x3)\n",
    "print('Train shape: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test shape: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
    "\n",
    "# plot first 9 images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(X_train[i])\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzRZ_dowt_Jv"
   },
   "source": [
    "## STEP 1: Load the data\n",
    "### 1.1. Load the dataset\n",
    "### 1.2. Reshape the inputs\n",
    "\n",
    "> Reshape the dataset inputs X_train and X_test to fit the model. Here 50000 is the number of samples, 32 is the width/height, 3-for the channels of color - Keras requires this 3rd dimension.\n",
    "\n",
    "### 1.3. One-hot encode the target column\n",
    "> E.g. if the input image contains a plane, the 3rd element of the target column is going to be 1 and the rest of the elements 0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6gwrVyTuD7W"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # TODO 1.1. load the dataset\n",
    "    (X_train, y_train), (X_test, y_test) = # TODO \n",
    "\n",
    "    # TODO 1.2. reshape the inputs (train and test)\n",
    "\n",
    "    # TODO 1.3. one-hot encode the target column (train and test)\n",
    "    \n",
    "    print(\"y_train[0] one hot: \", y_train[0])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGTSwT2VuKA6"
   },
   "source": [
    "## STEP 2: Prepare the data\n",
    "### 2.1. Convert the pixels \n",
    "> From integers to floats.\n",
    "\n",
    "### 2.2. Normalize the pixel values \n",
    "> From [0, 255] to [0.0, 1.0]. This is optional but in general using smaller values tends to give better results. Try also without this pre-processing and look at the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4P6Lla7uRa1"
   },
   "outputs": [],
   "source": [
    "def prepare_data(train, test):\n",
    "    # TODO 2.1. convert int to float\n",
    "    \n",
    "    # TODO 2.2. normalize to range [0.0, 1.0]\n",
    "\n",
    "    # return the normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "X_train, X_test = prepare_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kdGKXnKuWsH"
   },
   "source": [
    "## STEP 3: Build the model\n",
    "\n",
    "### 3.1. Create the model:\n",
    "> Sequential() is the easiest way to build a model in Keras. This allows you to build a model layer by layer. There is also Model() - class. Not in the scope of this lab.\n",
    "         \n",
    "         \n",
    "### 3.2. Add layers.\n",
    "> We can add the layers directly in the constructor (as a list) or later, using add().\n",
    "  - **L1**: a **convolutional layer** with **32 nodes**. Kernel size is 3 => **3x3 filter** matrix. Note that on the first conv layer we need to specify the input shape. The best parameters for this layer: **padding='same'**, **activation='relu'**.\n",
    "  - **L2**: In the second (also **conv**) layer we have **32 nodes** and the same parameters as above. \n",
    "  - **L3**: **Max Pooling** with **2x2 size** for the filter - can try without it and see what happens with the train time/loss.\n",
    "  - **L4**: Add a **dropout** layer - commonly used to prevent overfitting. Drop **1/4** of the neurons.\n",
    "  - **L5**: Add a **convolutional** layer with **64 neurons**, **kernel size 3**, **padding same** and **activation relu**.\n",
    "  - **L6**: Add a **convolutional** layer with **64 neurons**, **kernel size 3** and **activation relu**.\n",
    "  - **L7**: Add a **Max Pooling** layer with **filter size 2x2**.\n",
    "  - **L8**: Add a **Dropout** layer, with **1/4** of the neurons dropped.\n",
    "  - **L9**: Add a **Flatten** layer - the flatten layer serves as a connection b/w the conv and dense layers. Flatten = squeeze into 1D the activation maps and put them together. (Aka convert matrix to single array).\n",
    "  - **L10**: Add a **Dense** layer with **512** nodes, **relu** activation.\n",
    "  - **L11**: Add a **Dropout** layer, where **half** of the neurons are **dropped**.\n",
    "  - **L12**: Add a **Dense** layer. In this case the Dense layer is used for the output layer: **10 nodes**, for each possible outcome (0 - 10). The activation is **softmax** - makes the outputs to sum up to 1 => interpret them as probabilities.  The model is going to make its prediction based on which option has the highest probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s4wFBS5udv_"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "\n",
    "def build_model():\n",
    "    ### TODO 3.1. create the model\n",
    "    model = # TODO\n",
    "    \n",
    "    ### 3.2. add the layers\n",
    "    \n",
    "    # TODO L1: add a CONV  layer with 32 nodes, kernel size is 3, padding same, activation relu \n",
    "    \n",
    "    \n",
    "    # TODO L2: add a CONV layer with 32 nodes, kernel size is 3, activation relu\n",
    "\n",
    "    \n",
    "    # TODO L3: add a Max Pooling layer, 2x2\n",
    "\n",
    "    \n",
    "    # TODO L4: add a Dropout layer, drop 1/4 of the neurons\n",
    "\n",
    "    \n",
    "    # TODO L5: add a CONV layer with 64 neurons, kernel size is 3, padding same and the activation is relu\n",
    "\n",
    "    \n",
    "    # TODO L6: add a CONV layer with 64 nodes, kernel size is 3, activation relu\n",
    "\n",
    "    \n",
    "    # TODO L7: add a Max Pooling layer with filter size 2x2\n",
    "\n",
    "    \n",
    "    # TODO L8: add a Dropout layer; drop 1/4 of the neurons\n",
    "\n",
    "    \n",
    "    # TODO L9: add a Flatten layer\n",
    "\n",
    "    \n",
    "    # TODO L10: Add a Dense layer, 512 neurons and activation relu\n",
    "\n",
    "    \n",
    "    # TODO L11: Add a Dropout layer; drop 1/2 of the neurons\n",
    "\n",
    "    \n",
    "    # TODO L12: Add a Dense layer, with 10 neurons and softmax as activation\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69Q4cuiQuhTs"
   },
   "source": [
    "## STEP 4: Compile the model\n",
    "> Here we are going to need and optimizer, a loss function and a metric.\n",
    "  - **Optimizer** \n",
    "     - controls the learning rate. Here: **Adam** - a good optimizer for many cases.\n",
    "       Adam adjusts the learning rate throughout training.\n",
    "       Learning rate = how fast the optimal weights for the model are computed.\n",
    "       The LR value involves a trade-off b/w speed (larger lr) and accuracy(smaller lr).\n",
    "     - Other optimizers: https://keras.io/optimizers/  \n",
    "  - **Loss**\n",
    "     - **categorical_crossentropy** - a common choice for classification ( > 2 classes). The lower the score, the better.\n",
    "     - Other losses: https://keras.io/losses/ \n",
    "  - **Metric**\n",
    "     - **accuracy** - not the best choice but it is ok to make things easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s11nYnbmumxS"
   },
   "outputs": [],
   "source": [
    "# TODO 4. compile() the model using the adam optimizer, categorical crossentropy as loss and accuracy as metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gCQZ27Oux3m"
   },
   "source": [
    "## STEP 5: Train the model\n",
    "> Use the **fit()** function with the parameters:\n",
    "  - training data: **train_X** (images)\n",
    "  - target data: **train_y** (labels)\n",
    "  - **validation data**: **X_test**, **y_test** (periodically used during training to measure the performance of the model against data it has not seen before.)\n",
    "  - number of **epochs**: the number of times the model is cycling through the data. Not to large as the model only improves up to a certain point. Set to **100** in this case.\n",
    "  - **shuffle** the data for batches.\n",
    "  - a **batch size** of **64** is ok. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3nG-LAVuzM2"
   },
   "outputs": [],
   "source": [
    "# TODO 5. Fit the model with the training data, validation data, 100 epochs, bach size 64, with shuffle\n",
    "history = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dp7AEvajUtQf"
   },
   "source": [
    "## STEP 6: Save the model\n",
    "\n",
    "From this model we are interested in the weights. Thus, in order to use the model later, we need to save the weights learned at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZpsSpCvxVZBH"
   },
   "outputs": [],
   "source": [
    "model.save_weights('cifar_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ic5p6pvVjja"
   },
   "source": [
    "## STEP 7: Make predictions\n",
    "> Use the **predict()** function to see the actual predictions that our model has made for the test data.The predict() function gives an array with 10 numbers - the probaility that th input is 0-9. The array index with the highest value is the prediction made by the model.\n",
    "\n",
    "> Additionally: re-load the trained model. Essentially what we have to do here is to rebuild the model and load in the saved weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrF8EjhKhzfA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_test_model():\n",
    "    # TODO 7.1. (re)build the model\n",
    "    \n",
    "    # TODO 7.2. load the model's saved weights.\n",
    "\n",
    "    # TODO 7.3. Predict the class of the first 10 images in the test set\n",
    "    predictions = # TODO\n",
    "    \n",
    "    # 7.4. Print the predictions done by the model ([7, 2, 1, 0, 4])\n",
    "    print(\"Predictions: \\n\", np.argmax(predictions, axis=1))\n",
    "    \n",
    "    # 7.5. Check our predictions against the ground truths ([7, 2, 1, 0, 4])\n",
    "    print(\"Ground truth: \\n\", y_test[:10])\n",
    "    \n",
    "load_test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEgfts1xXVWG"
   },
   "source": [
    "## STEP 8: Learning Curves\n",
    "\n",
    "Plot the learning curves to get an idea about how the model learns, overfitting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73HP0D3ahzfT"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "\t  plt.subplot(211)\n",
    "\t  plt.title('Cross Entropy Loss')\n",
    "\t  plt.plot(history.history['loss'], color='blue', label='train')\n",
    "\t  plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    \n",
    "\t  # plot accuracy\n",
    "\t  plt.subplot(212)\n",
    "\t  plt.title('Classification Accuracy')\n",
    "\t  plt.plot(history.history['acc'], color='blue', label='train')\n",
    "\t  plt.plot(history.history['val_acc'], color='orange', label='test')\n",
    "    \n",
    "\t  # save plot to file\n",
    "\t  filename = sys.argv[0].split('/')[-1]\n",
    "\t  plt.savefig(filename + '_plot.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# evaluate model\n",
    "_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "# learning curves\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSj-9lz-YXXG"
   },
   "source": [
    "## BONUS\n",
    "\n",
    "With the knowledge achieved through this lab, build a simplified CNN which is able to classify the digits in the MNIST dataset. The network does not need to have more than 2 convolutional layers in order for the performance to be satisfactory in terms of accuracy.\n",
    "\n",
    "Note that the MNIST dataset is lighter than CIFAR as the images are grayscale (only one color channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kb_ZDV04hW3L"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO B1. Download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = # TODO\n",
    "\n",
    "# Plot the first image in the dataset\n",
    "plt.imshow(X_train[0])\n",
    "\n",
    "# Check the image shape\n",
    "print(\"image shape: \", X_train[0].shape)\n",
    "\n",
    "######################\n",
    "# B2. Prepare the data\n",
    "######################\n",
    "\n",
    "# TODO B2.1. Normalize the pixel values from [0, 255] to [-0.5, 0.5].\n",
    "\n",
    "\n",
    "# TODO B2.2. Reshape the dataset inputs X_train and X_test to fit the model \n",
    "# train: (60000, 28, 28, 1), test: (10000, 28, 28, 1)\n",
    "\n",
    "\n",
    "# TODO B2.3. One-hot encode the target column\n",
    "\n",
    "print(\"y_train[0] one hot: \", y_train[0])\n",
    "\n",
    "#####################\n",
    "# B3. Build the model\n",
    "#####################\n",
    "\n",
    "# TODO B3.1. Create model\n",
    "\n",
    "# TODO B3.2. Add conv layer - 64 nodes, 3x3, relu\n",
    "\n",
    "# TODO B3.3. Add conv layer - 32 nodes, 3x3, relu\n",
    "\n",
    "# TODO B3.4. Add max pooling layer - 2x2 pool size\n",
    "\n",
    "# TODO B3.5. Add a dropout layer - 0.5\n",
    "\n",
    "# TODO B3.6. Aadd flatten layer\n",
    "\n",
    "# TODO B3.7. Add dense layer - 10 nodes, softmax\n",
    "\n",
    "#######################\n",
    "# B4. Compile the model\n",
    "#######################\n",
    "\n",
    "# TODO B4. Optimizer: adam, Loss: categorical_crossentropy, Metric: accuracy.\n",
    "\n",
    "#####################\n",
    "# B5. Train the model\n",
    "#####################\n",
    "\n",
    "# TODO B5. Use the fit() function with the parameters:\n",
    "#   - training data: train_X (images)\n",
    "#   - target data: train_y (labels)\n",
    "#   - validation data: X_test, y_test \n",
    "#   - number of epochs: 3\n",
    "history = # TODO\n",
    "\n",
    "######################\n",
    "# B6. Make predictions\n",
    "######################\n",
    "\n",
    "# Predict\n",
    "print(\"Predictions for the first 4 test samples: \\n\", model.predict(X_test[:4]))\n",
    "\n",
    "# Compare with the actual results\n",
    "print(\"Actual values in the test set: \\n\", y_test[:4])\n",
    "\n",
    "#####################\n",
    "# B7. Using the model\n",
    "#####################\n",
    "\n",
    "# TODO B7. Save model\n",
    "\n",
    "##############################\n",
    "# B8. Plot the learning curves\n",
    "##############################\n",
    "\n",
    "# TODO B8.1. Evaluate model\n",
    "_, acc = # TODO\n",
    "\n",
    "print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "# TODO B8.2. Plot the learning curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzeJBRILjRib"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_1_CNN_Cifar_SKEL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
